<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ChatBridge: Bridging Modalities with Large Language Model as a
              Language Catalyst</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Zijia Zhao</a><sup>1</sup>,</span>
              <span class="author-block">
                Longteng Guo</a><sup>2</sup>,</span>
              <span class="author-block">
                Tongtian Yue</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Sihan Chen</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Shuai Shao</a><sup>2</sup>,
              </span>
              <span class="author-block">
                Xinxin Zhu</a><sup>1</sup>,
              </span>
              <span class="author-block">
                Zehuan Yuan</a><sup>2</sup>,
              </span>
              <span class="author-block">
                jing Liu</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Institute of Automation, Chinese Academy of Sciences,</span>
              <span class="author-block"><sup>2</sup>Bytedance Inc.</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2305.16103.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2305.16103" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/joez17/ChatBridge" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/joez17/ChatBridge" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

  <!-- 
<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">

          <h2 class="title is-3">Abstract</h2>
          <img id="teaser" width="40%" src="static/examples/ills.png">
          
          <div class="content has-text-justified">
            <p>
              Building general-purpose models that can perceive diverse real-world modalities and
              solve various tasks is an appealing target in artificial intelligence.
            </p>
            <p>
              In this paper, we present ChatBridge, a novel multimodal language model that
              leverages the expressive capabilities of language as the catalyst to bridge the
              gap between various modalities. We show that only language-paired two-modality
              data is sufficient to connect all modalities.
              ChatBridge leverages recent large language models (LLM) and extends their zero-shot capabilities to
              incorporate diverse multimodal inputs.
            </p>
            <p>
              ChatBridge undergoes a two-stage training. The first stage aligns each modality with language,
              which brings emergent multimodal correlation and collaboration abilities.
              The second stage instruction-finetunes ChatBridge to align it with user intent with our newly
              proposed multimodal instruction tuning dataset, named MULTIS, which covers a wide range of 16 multimodal
              tasks of text, image, video, and audio modalities.
            </p>
            <p>
              We show strong quantitative and qualitative results on zero-shot multimodal tasks covering text, image,
              video, and audio modalities.
              All codes, data, and models of ChatBridge will be open-sourced.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
      <!--/ Paper video. -->
    </div>
  </section>




  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="3%" src="static/images/icon.png"> ChatBridge</h2>
      </div>
    </div>
    <!-- </div> https://cdn-icons-png.flaticon.com/512/5379/5379860.png -->
    <!--/ Results. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              ChatBridge is a multimodal language model capable of perceiving real-world multimodal information,
              as well as following instructions, thinking, and interacting with humans in natural language.
              Inspired by <a href="https://arxiv.org/abs/2204.14198">Flamingo</a> and <a
                href="https://arxiv.org/abs/2301.12597">BLIP-2</a>,
              we introduce perceiver modules to bridge the encoders and the LLM.
              we choose open-sourced <a href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna-13B</a> as the LLM,
              which is built upon LLaMA, and reported to achieve 90% of ChatGPT's quality as per GPT-4's evaluation.
              As for the modal-specific encoders, we choose <a href="https://arxiv.org/abs/2211.07636">EVA-ViT-G</a> as
              the vision encoder to encode images and videos,
              and <a href="https://arxiv.org/abs/2212.09058">BEAT</a> as the audio encoder to encoder audios.

              <ul type="1">
                <li><b>Stage 1: Bridge each modality with language</b>, <span style="font-size: 95%;">leverage
                    large-scale language-paired two-modality data for multimodal
                    alignment training, including image-text, video-text, and audio-text pairs.</span></li>
                <li><b>Stage 2: Multimodal Instruction Tuning</b>, <span style="font-size: 95%;">instruction-finetune
                    ChatBridge to align the model with user intent on a
                    multimodal instruction dataset <b>MULTIS</b>, enabling more effective zero-shot generalization on
                    multimodal tasks.</span></li>
              </ul>

            </p>
          </div>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="static/images/arch.png">
            </div>


          </centering>
        </div>
      </div>



  </section>


  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="3%" src="static/images/data.png"> MULTimodal InStruction
          Datasets (MULTIS)</h2>
      </div>
    </div>
    <!-- </div> https://cdn-icons-png.flaticon.com/512/5379/5379860.png -->
    <!--/ Results. -->
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              MULTIS consists of two distinct parts: task-specific data and multimodal chat data.
              The whole collection of MULTIS covers 16 multimodal task categories and 15 source datasets.
              <ul type="1">
                <li><b>Task-Specific Data</b>. <span style="font-size: 95%;">We collect a vast array of publicly
                    available human-annotated multimodal datasets
                    and transform them into a unified instruction tuning format.</span></li>
                <li><b>Multimodal Chat Data</b>. <span style="font-size: 95%;">While task-specific data empowers the
                    model towards completing standardized tasks,
                    multimodal chat data offers real-world, open-ended dialogues demanding more sophisticated intent
                    comprehension and contextual reasoning abilities,
                    as well as providing more diverse, helpful, human-like responses. Despite the image-to-text chat
                    dataset generated by
                    <a href="https://arxiv.org/abs/2304.08485">LLaVA-Instruct-150K</a>,
                    chat data across other modalities remains limited. To this end, we have constructed a multimodal
                    chat dataset that comprises both unimodal
                    and multimodal inputs of image, video, and audio modalities.</span></li>
              </ul>
            </p>
          </div>
          <centering>
            <div style="text-align: center;">
              <img id="teaser" width="90%" src="static/images/gen.png">
            </div>


          </centering>
        </div>
      </div>



  </section>






  <section class="section">
    <!-- Results. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-six-fifths">
        <h2 class="title is-3"><img id="painting_icon" width="3%" src="static/images/topic.png"> Examples of multimodal
          chat</h2>
      </div>
    </div>
    <div class="container is-max-desktop">

      <div class="columns is-centered">
        <div class="column is-full-width">
          <div class="content has-text-justified">
            <p>
              <!-- sample -->
          </div>
          <centering>
            <div style="text-align: center;">
              <h2 class="title is-5"> Audio+Video Input</h2>
              <img id="teaser" width="90%" src="static/examples/av.png">
              <br></br>
              <h2 class="title is-5"> Audio+Image Input</h2>
              <img id="teaser" width="90%" src="static/examples/a1.png">
              <br></br>
              <h2 class="title is-5"> Image Input</h2>
              <img id="teaser" width="90%" src="static/examples/i.png">
              <br></br>
              <h2 class="title is-5"> Audio Input</h2>
              <img id="teaser" width="90%" src="static/examples/a.png">
              <br></br>
              <h2 class="title is-5"> Video Input</h2>
              <img id="teaser" width="90%" src="static/examples/v.png">
              <br></br>
            </div>
        </div>




          </div>



  </section>



  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhao2023chatbridge,
        title={ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst},
        author={Zhao, Zijia and Guo, Longteng and Yue, Tongtian and Chen, Sihan and Shao, Shuai and Zhu, Xinxin and Yuan, Zehuan and Liu, Jing},
        journal={arXiv preprint arXiv:2305.16103},
        year={2023}
      }</code></pre>
    </div>
  </section>
  
  <section class="section" id="Acknowledgement">
    <div class="container is-max-desktop content">
      <h2 class="title">Acknowledgement</h2>
      <p>
        This website is adapted from <a
        href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>, licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>. 
      </p>

    </div>
  </section>

</body>

</html>